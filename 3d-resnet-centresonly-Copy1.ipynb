{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common code and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN THIS IF THE GPU SERVER IS BUSY\n",
    "#USES THE SLOWASS CPU INSTEAD\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to choose 2nd GPU\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if the following error is thrown when training/testing models:\n",
    "\n",
    "# Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, \n",
    "# so try looking to see if a warning log message was printed above.\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from keras.backend import manual_variable_initialization\n",
    "\n",
    "import gc\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config_tf()\n",
    "\n",
    "\n",
    "def config_tf():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "    \n",
    "config_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import SimpleITK as sitk\n",
    "import os, sys\n",
    "#sys.path.insert(1, './Models/Resnet-3D')\n",
    "from resnet3d4 import Resnet3DBuilder\n",
    "import resnet3d4\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "import math\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json #, load_model\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.ndimage.morphology import binary_fill_holes\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#these are imported or the transferring models are not deserialised properly\n",
    "import conv4d\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#if this is not here, the loaded notebook will not detect ensuing changes in imported numpy scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jzhe0882/numpydata/HeadNeckCancer/PET/HN-CHUS-048.npy /home/jzhe0882/numpydata/HeadNeckCancer/CT/HN-CHUS-048.npy /home/jzhe0882/Radiomics/HeadNeckCancer/PET/HN-CHUS-048.npy /home/jzhe0882/Radiomics/HeadNeckCancer/CT/HN-CHUS-048.npy [ 64  49 105] [9 8 7] /home/jzhe0882/numpydata/HeadNeckCancer/Mask/HN-CHUS-048.npy\n",
      "/home/jzhe0882/numpydata/HeadNeckCancer/PET/HN-HGJ-035.npy /home/jzhe0882/numpydata/HeadNeckCancer/CT/HN-HGJ-035.npy /home/jzhe0882/Radiomics/HeadNeckCancer/PET/HN-HGJ-035.npy /home/jzhe0882/Radiomics/HeadNeckCancer/CT/HN-HGJ-035.npy [70 38 61] [16 14 33] /home/jzhe0882/numpydata/HeadNeckCancer/Mask/HN-HGJ-035.npy\n",
      "/home/jzhe0882/numpydata/BreastCancer/PET/10368550.npy /home/jzhe0882/numpydata/BreastCancer/CT/10368550.npy /home/jzhe0882/Radiomics/BreastCancer/PET/10368550.npy /home/jzhe0882/Radiomics/BreastCancer/CT/10368550.npy [46 58 62] [3 3 4] /home/jzhe0882/numpydata/BreastCancer/Mask/10368550.npy\n",
      "/home/jzhe0882/numpydata/BreastCancer/PET/10359092.npy /home/jzhe0882/numpydata/BreastCancer/CT/10359092.npy /home/jzhe0882/Radiomics/BreastCancer/PET/10359092.npy /home/jzhe0882/Radiomics/BreastCancer/CT/10359092.npy [77 61 79] [4 4 7] /home/jzhe0882/numpydata/BreastCancer/Mask/10359092.npy\n"
     ]
    }
   ],
   "source": [
    "#load training data from disk\n",
    "\n",
    "pet_data, ct_data, centre_data, size_data, mask_data, ct_radiomics_data, pet_radiomics_data = {'train':{}, 'test':{}}, {'train':{}, 'test':{}}, \\\n",
    "                                                        {'train':{}, 'test':{}}, {'train':{}, 'test':{}}, {'train':{}, 'test':{}}, \\\n",
    "                                                        {'train':{}, 'test':{}}, {'train':{}, 'test':{}}\n",
    "\n",
    "for dataset in ['HeadNeckCancer', 'BreastCancer']:\n",
    "    pet_files = []\n",
    "    ct_files = []\n",
    "    centre_files = []\n",
    "    size_files = []\n",
    "    mask_files = []\n",
    "    pet_radiomics_files = []\n",
    "    ct_radiomics_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/numpydata/' + dataset + '/PET'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            pet_files.append(file_path)\n",
    "\n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/numpydata/' + dataset + '/CT'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            ct_files.append(file_path)\n",
    "\n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/numpydata/' + dataset + '/MaskCentres'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            centre_files.append(file_path)\n",
    "\n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/numpydata/' + dataset + '/MaskSizes'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            size_files.append(file_path)\n",
    "\n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/numpydata/' + dataset + '/Mask'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            mask_files.append(file_path)\n",
    "            \n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/Radiomics/' + dataset + '/CT'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            ct_radiomics_files.append(file_path)\n",
    "            \n",
    "    for root, dirs, files in os.walk('/home/jzhe0882/Radiomics/' + dataset + '/PET'):\n",
    "        for name in files:\n",
    "            file_path = os.path.join(root, name)\n",
    "            pet_radiomics_files.append(file_path)\n",
    "\n",
    "    pet_files = sorted(pet_files)\n",
    "    ct_files = sorted(ct_files)\n",
    "    centre_files = sorted(centre_files)\n",
    "    centres = [np.load(c) for c in centre_files] #can load all of these into memory (other volumes are too large)\n",
    "    size_files = sorted(size_files)\n",
    "    bounding_box_sizes = [np.load(s) for s in size_files] #can also load all of these into memory\n",
    "    mask_files = sorted(mask_files)\n",
    "    pet_radiomics_files= sorted(pet_radiomics_files)\n",
    "    ct_radiomics_files = sorted(ct_radiomics_files)\n",
    "\n",
    "    #Inputs are PET/CT data, outputs are centres, sizes, masks\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(zip(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files)), \n",
    "                                                        list(zip(centres, bounding_box_sizes, mask_files)), \n",
    "                                                        test_size=0.33, shuffle=True, random_state=9)\n",
    "    \n",
    "    pet_data['train'][dataset], ct_data['train'][dataset], pet_radiomics_data['train'][dataset], ct_radiomics_data['train'][dataset] = zip(*X_train)\n",
    "    pet_data['test'][dataset], ct_data['test'][dataset], pet_radiomics_data['test'][dataset], ct_radiomics_data['test'][dataset] = zip(*X_test)\n",
    "    centre_data['train'][dataset], size_data['train'][dataset], mask_data['train'][dataset] = zip(*y_train)\n",
    "    centre_data['test'][dataset], size_data['test'][dataset], mask_data['test'][dataset] = zip(*y_test)\n",
    "\n",
    "    print(pet_data['train'][dataset][0], ct_data['train'][dataset][0], pet_radiomics_data['train'][dataset][0], ct_radiomics_data['train'][dataset][0],\n",
    "          centre_data['train'][dataset][0], size_data['train'][dataset][0], mask_data['train'][dataset][0])\n",
    "    \n",
    "    print(pet_data['test'][dataset][0], ct_data['test'][dataset][0], pet_radiomics_data['test'][dataset][0], ct_radiomics_data['test'][dataset][0],\n",
    "          centre_data['test'][dataset][0], size_data['test'][dataset][0], mask_data['test'][dataset][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model_function(filepath, model):\n",
    "    model.save_weights(filepath + '.h5')\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(filepath + '.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "def load_model_function(filepath):\n",
    "    # load json and create model\n",
    "    with open(filepath + '.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        \n",
    "    loaded_model = model_from_json(loaded_model_json, custom_objects=custom_objects)\n",
    "\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(filepath + '.h5')\n",
    "    return loaded_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x,y,z) + (x,y,z) => (x,y,z, 2)\n",
    "def zip_np_volumes(vol_a, vol_b):\n",
    "    vol_a = np.expand_dims(np.array(vol_a), axis=-1)\n",
    "    vol_b = np.expand_dims(np.array(vol_b), axis=-1)\n",
    "    return np.concatenate((vol_a, vol_b), axis=-1)\n",
    "\n",
    "def normalise_volume(vol):\n",
    "    std = np.std(vol)\n",
    "    if np.isclose(std, 0):\n",
    "        std = 1\n",
    "    vol = np.divide(vol - np.mean(vol), std)\n",
    "    return vol\n",
    "\n",
    "volume_size = np.array([128,128,128], dtype=int)\n",
    "def shift_centre(centre, shift_weight, seed=None):\n",
    "    np.random.seed(seed) \n",
    "    \n",
    "    centre = np.random.randn(3) * shift_weight * np.array([19.69, 4.24, 13.83]) + centre #std taken from NumpyAnalysis\n",
    "    centre = np.maximum(centre, [0,0,0]) #keep centre within image bounds\n",
    "    centre = np.minimum(centre, volume_size)\n",
    "    return centre.astype(int)\n",
    "\n",
    "def get_volume_input(modalities, ct_file, pet_file, ct_radiomics_file, pet_radiomics_file, volume_shape, use_radiomics):\n",
    "    ct_pet = np.empty(volume_shape + (0,))\n",
    "                \n",
    "    if 'ct' in modalities:\n",
    "        ct = np.expand_dims(np.load(ct_file), axis=-1)\n",
    "        ct_pet = np.append(ct_pet, ct, axis=-1)\n",
    "    if 'pet' in modalities:\n",
    "        pet = np.expand_dims(np.load(pet_file), axis=-1)\n",
    "        ct_pet = np.append(ct_pet, pet, axis=-1)\n",
    "\n",
    "    if use_radiomics is not None:\n",
    "        if 'ct' in modalities:\n",
    "            ct_radiomics = np.expand_dims(np.load(ct_radiomics_file)[:,:,:,use_radiomics], axis=-1)\n",
    "            ct_pet = np.concatenate((ct_pet, ct_radiomics), axis=-1)\n",
    "\n",
    "        if 'pet' in modalities:\n",
    "            pet_radiomics_files = np.expand_dims(np.load(pet_radiomics_file)[:,:,:,use_radiomics], axis=-1)\n",
    "            ct_pet = np.concatenate((ct_pet, pet_radiomics_files), axis=-1)\n",
    "\n",
    "    return ct_pet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#source_volume is a 3-dim (w,h,d) or 4-dim array (w,h,d,channels)\n",
    "#generates a bounding box volume given the parameters\n",
    "#centre is assumed to be relative to the source volume and rounded down\n",
    "#extents is size of the bounding_box / 2\n",
    "def get_bounding_box(source_volume, centre, extents):\n",
    "    extents_ceil = np.ceil(extents).astype(int)\n",
    "    extents_floor = extents.astype(int)\n",
    "    centre = np.rint(centre).astype(int)\n",
    "    \n",
    "    maxima = centre + extents_ceil\n",
    "    minima = centre - extents_floor\n",
    "    \n",
    "    #keep bounding box dimensions within the mask dimensions\n",
    "    maxima = np.minimum(maxima, np.array(source_volume.shape)[:3]).astype(int)\n",
    "    minima = np.maximum(minima, [0,0,0]).astype(int) \n",
    "        \n",
    "    bounding_box_values = source_volume[minima[0]:maxima[0],\n",
    "                                      minima[1]:maxima[1],\n",
    "                                      minima[2]:maxima[2]]\n",
    "    \n",
    "    relative_centre = extents_floor\n",
    "    relative_maxima = relative_centre + maxima - centre\n",
    "    relative_minima = relative_centre + minima - centre\n",
    "    \n",
    "    bbox_shape = tuple(extents_ceil + extents_floor)\n",
    "    if len(source_volume.shape) == 4:\n",
    "        bbox_shape += (source_volume.shape[3],)\n",
    "    bounding_box = np.zeros(bbox_shape)\n",
    "    \n",
    "    bounding_box[relative_minima[0]:relative_maxima[0],\n",
    "                relative_minima[1]:relative_maxima[1],\n",
    "                relative_minima[2]:relative_maxima[2]] = bounding_box_values\n",
    "    \n",
    "    return bounding_box\n",
    "\n",
    "#translates the bounding box so that its values have a new reference centre\n",
    "def align_bounding_box(bounding_box, box_centre, target_centre):\n",
    "    displacement = (target_centre - box_centre).astype(int)\n",
    "    new_box = np.copy(bounding_box)\n",
    "    \n",
    "    #boxes to be shifted forward have a trails of zeroes at the end of the array\n",
    "    #boxes to be shifted backward have a trails of zeroes at the beginning of the array\n",
    "    if displacement[0] < 0:\n",
    "        new_box[displacement[0]:, :, :] = 0\n",
    "    else:\n",
    "        new_box[:displacement[0], :, :] = 0\n",
    "                      \n",
    "    if displacement[1] < 0:\n",
    "        new_box[:, displacement[1]:, :] = 0\n",
    "    else:\n",
    "        new_box[:, :displacement[1], :] = 0\n",
    "    \n",
    "    if displacement[2] < 0:\n",
    "        new_box[:, :, displacement[2]:] = 0\n",
    "    else:\n",
    "        new_box[:, :, :displacement[2]] = 0\n",
    "    \n",
    "    new_box = np.roll(new_box, -displacement, axis=(0,1,2))\n",
    "    return new_box\n",
    "\n",
    "#resample bounding_box to target_size while keeping aspect ratios\n",
    "def resize_bounding_box(bounding_box, target_size, mode):\n",
    "    size_ratio = np.amin(np.divide(target_size, bounding_box.shape))\n",
    "    size_ratio_index = np.argmin(np.divide(target_size, bounding_box.shape))\n",
    "    \n",
    "    #the resampling here is slightly inaccurate since new_size is rounded to the nearest int\n",
    "    bounding_box = sitk.GetImageFromArray(np.transpose(bounding_box))\n",
    "    new_size = np.rint(size_ratio * np.array(bounding_box.GetSize()))\n",
    "    new_spacing = tuple(np.multiply(np.divide(bounding_box.GetSpacing(), size_ratio), np.divide(new_size, new_size+1)))\n",
    "    #additionally multiply spacing by size/(size+1) to fill out an extra row of values\n",
    "        \n",
    "    assert new_size[size_ratio_index] == target_size[size_ratio_index]\n",
    "    \n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetReferenceImage(bounding_box)\n",
    "    resampler.SetSize((int(new_size[0]), int(new_size[1]), int(new_size[2])))\n",
    "    resampler.SetOutputSpacing(new_spacing)\n",
    "\n",
    "    if mode == 'mask':\n",
    "        resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resampler.SetInterpolator(sitk.sitkLinear)\n",
    "        \n",
    "    new_box = np.transpose(sitk.GetArrayFromImage(resampler.Execute(bounding_box)))\n",
    "    \n",
    "    #pads the surroundings with 0's\n",
    "    return get_bounding_box(new_box, 0.5 * new_size, 0.5 * target_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generator for (ct,pet)->centre prediction models\n",
    "def detection_generator(mode='train', dataset='HeadNeckCancer', shuffle=True, normalise=True, use_radiomics=True, \n",
    "                        modalities=['ct','pet'], batch_size=4):\n",
    "    \n",
    "    def helper(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres):\n",
    "        num_data = len(pet_files)\n",
    "        volume_shape = np.load(ct_files[0]).shape\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            if shuffle:\n",
    "                z = list(zip(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres))\n",
    "                random.shuffle(z)\n",
    "                pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres = zip(*z)\n",
    "\n",
    "            ct_pet_batch = []\n",
    "            centre_batch = []\n",
    "            \n",
    "            for i in range(num_data):\n",
    "                ct_pet = get_volume_input(modalities, ct_files[i], pet_files[i], ct_radiomics_files[i], pet_radiomics_files[i], \n",
    "                                          volume_shape, use_radiomics)\n",
    "                    \n",
    "                if normalise:                    \n",
    "                    for axis in range(ct_pet.shape[-1]):\n",
    "                        ct_pet[:,:,:,axis] = normalise_volume(ct_pet[:,:,:,axis])\n",
    "                \n",
    "                ct_pet_batch.append(ct_pet)\n",
    "                centre_batch.append(centres[i])\n",
    "\n",
    "                if len(ct_pet_batch) == batch_size:\n",
    "                    yield np.array(ct_pet_batch), np.array(centre_batch)\n",
    "\n",
    "                    ct_pet_batch.clear()\n",
    "                    centre_batch.clear()\n",
    "\n",
    "            if len(ct_pet_batch) > 0:\n",
    "                yield np.array(ct_pet_batch), np.array(centre_batch)\n",
    "    \n",
    "    return helper(pet_data[mode][dataset], ct_data[mode][dataset], pet_radiomics_data[mode][dataset], ct_radiomics_data[mode][dataset],\n",
    "                  centre_data[mode][dataset])\n",
    "\n",
    "# generator for (ct,pet,centre)->(centre,size) prediction models\n",
    "def localisation_generator(mode='train', dataset='HeadNeckCancer', shuffle=True, normalise=True, use_radiomics=None, batch_size=4, \n",
    "                           shift_weight=0.7, pred_centres=None, region_radius=[16,16,16], modalities=['ct', 'pet']):\n",
    "    \n",
    "    def helper(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, shuffle, normalise, use_radiomics, batch_size):\n",
    "        num_data = len(pet_files)\n",
    "        volume_shape = np.load(ct_files[0]).shape\n",
    "        val_seeds = list(range(num_data))\n",
    "        nonlocal pred_centres\n",
    "        if pred_centres is None:\n",
    "            pred_centres = [None] * num_data\n",
    "\n",
    "        while True: #loops once per epoch\n",
    "            if shuffle:\n",
    "                z = list(zip(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, pred_centres, val_seeds))\n",
    "                random.shuffle(z)\n",
    "                pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, pred_centres, val_seeds = zip(*z)\n",
    "\n",
    "            ct_pet_batch = []\n",
    "            shift_centre_batch = []\n",
    "            true_centre_size_batch = []\n",
    "            \n",
    "            for i in range(num_data):\n",
    "                ct_pet = get_volume_input(modalities, ct_files[i], pet_files[i], ct_radiomics_files[i], pet_radiomics_files[i], \n",
    "                                          volume_shape, use_radiomics)\n",
    "                    \n",
    "                if normalise:                    \n",
    "                    for axis in range(ct_pet.shape[-1]):\n",
    "                        ct_pet[:,:,:,axis] = normalise_volume(ct_pet[:,:,:,axis])\n",
    "                \n",
    "                if pred_centres[i] is None: # generate random centres\n",
    "                    if mode == 'test': #make the validation generator generate predictable centres\n",
    "                        seed = val_seeds[i]\n",
    "                    elif mode == 'train':\n",
    "                        seed = None\n",
    "                    shifted_centre = shift_centre(centres[i], shift_weight, seed)\n",
    "                else:\n",
    "                    shifted_centre = pred_centres[i] #use the user-defined centre\n",
    "                    \n",
    "                ct_pet = get_bounding_box(ct_pet, shifted_centre, np.array(region_radius))\n",
    "                                \n",
    "                ct_pet_batch.append(ct_pet)\n",
    "                shift_centre_batch.append(shifted_centre)\n",
    "                true_centre_size_batch.append(centres[i])\n",
    "\n",
    "                if len(ct_pet_batch) == batch_size:\n",
    "                    yield [np.array(ct_pet_batch), np.array(shift_centre_batch)], np.array(true_centre_size_batch)\n",
    "\n",
    "                    ct_pet_batch.clear()\n",
    "                    shift_centre_batch.clear()\n",
    "                    true_centre_size_batch.clear()\n",
    "\n",
    "            if len(ct_pet_batch) > 0:\n",
    "                yield [np.array(ct_pet_batch), np.array(shift_centre_batch)], np.array(true_centre_size_batch)\n",
    "    \n",
    "    return helper(pet_data[mode][dataset], ct_data[mode][dataset], pet_radiomics_data[mode][dataset], ct_radiomics_data[mode][dataset],\n",
    "                  centre_data[mode][dataset], size_data[mode][dataset], shuffle, normalise, use_radiomics, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 batches of 10 samples taken over 98 total samples\n",
      "0 (10, 32, 32, 32, 2) (10, 3) (10, 3)\n",
      "1 (10, 32, 32, 32, 2) (10, 3) (10, 3)\n",
      "0 (10, 128, 128, 128, 2) (10, 3)\n",
      "1 (10, 128, 128, 128, 2) (10, 3)\n"
     ]
    }
   ],
   "source": [
    "#test to see if generator works\n",
    "\n",
    "batch_size = 10\n",
    "dataset = 'HeadNeckCancer'\n",
    "mode = 'test'\n",
    "test_generator = localisation_generator(mode=mode, dataset=dataset, batch_size=batch_size, shuffle=True, use_radiomics=1, modalities=['pet'])\n",
    "print('{} batches of {} samples taken over {} total samples'.format(\n",
    "    math.ceil(len(ct_data[mode][dataset])/batch_size), batch_size, len(ct_data[mode][dataset])))\n",
    "\n",
    "for i in range(2):#math.ceil(len(ct_data[mode][dataset])/batch_size)):\n",
    "    ctpet, centresize = next(test_generator)\n",
    "    print(i, ctpet[0].shape, ctpet[1].shape, centresize.shape)\n",
    "    \n",
    "test_generator = detection_generator(mode=mode, dataset=dataset, batch_size=batch_size, shuffle=True, use_radiomics=1, modalities=['ct'])\n",
    "for i in range(2):#math.ceil(len(ct_data[mode][dataset])/batch_size)):\n",
    "    ctpet, centre = next(test_generator)\n",
    "    print(i, ctpet.shape, centre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate detection model\n",
    "\n",
    "def evaluate_detection_model(model, dataset, function='detection', batch_size=4, use_radiomics=None, modalities=['ct', 'pet']):\n",
    "    mode = 'test'\n",
    "    \n",
    "    if function == 'detection':\n",
    "        test_generator = detection_generator(mode=mode, dataset=dataset, shuffle=False, normalise=True, batch_size=batch_size, \n",
    "                                             use_radiomics=use_radiomics, modalities=modalities)    \n",
    "        detected_centres = model.predict_generator(test_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size))\n",
    "        \n",
    "        return str(mean_squared_error(detected_centres, centre_data[mode][dataset], multioutput='raw_values'))\n",
    "    \n",
    "    elif function == 'localisation':\n",
    "        \n",
    "        #validation_generator = localisation_generator(mode='test', dataset=dataset, batch_size=batch_size, shuffle=False, normalise=True, \n",
    "        #                                       use_radiomics=use_radiomics)\n",
    "        #print(model.evaluate_generator(validation_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size)))\n",
    "        \n",
    "        test_generator = localisation_generator(mode=mode, dataset=dataset, shuffle=False, normalise=True, batch_size=batch_size,\n",
    "                                               use_radiomics=use_radiomics, modalities=modalities)    \n",
    "        detected_centres = model.predict_generator(test_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size))\n",
    "\n",
    "        centre_mse =  mean_squared_error(detected_centres, centre_data[mode][dataset], multioutput='raw_values')\n",
    "        \n",
    "        return str(centre_mse)\n",
    "    \n",
    "#print(evaluate_detection_model(localisation_model, 'BreastCancer', function='localisation'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#cycles_per_epoch = how many times the entire training set should be cycled over for each epoch\n",
    "#total_cycles = how many times the entire training set should be cycled in total\n",
    "\n",
    "def train_detection_model(dataset, save_model=True, batch_size=4, total_epochs=100, normalise=True, shuffle=True, use_shortcut=True,\n",
    "                          use_radiomics=None, transferring_model=None, loss=losses.mean_squared_error, function='detection', modalities=['ct', 'pet']):\n",
    "\n",
    "    def get_model_name():\n",
    "        return '{} {} batch={} cycles={} modalities=[] use_radiomics={} transferred={} shortcut={} {}'.format(\n",
    "            function, dataset, batch_size, total_epochs, modalities, use_radiomics, transferring_model!=None, use_shortcut, loss.__name__)\n",
    "        \n",
    "    #define input shapes\n",
    "    num_channels = len(modalities)\n",
    "    if use_radiomics is not None:\n",
    "        num_channels *= 2\n",
    "        \n",
    "    if function == 'detection':\n",
    "        detection_input_shape = (128, 128, 128, num_channels)\n",
    "        generator_funct = detection_generator\n",
    "            \n",
    "    elif function == 'localisation':\n",
    "        detection_input_shape = [(32, 32, 32, num_channels), (3,)]\n",
    "        generator_funct = localisation_generator\n",
    "\n",
    "    train_generator = generator_funct(mode='train', dataset=dataset, batch_size=batch_size, shuffle=shuffle, normalise=normalise,\n",
    "                                          use_radiomics=use_radiomics, modalities=modalities)\n",
    "    validation_generator = generator_funct(mode='test', dataset=dataset, batch_size=batch_size, shuffle=False, normalise=normalise, \n",
    "                                               use_radiomics=use_radiomics, modalities=modalities)\n",
    "\n",
    "    #determine whether to use old model or build new one from scratch\n",
    "    if transferring_model is not None:\n",
    "        detection_model = transferring_model\n",
    "        #for source_layer, target_layer in zip(transferring_model.layers, detection_model.layers):\n",
    "         #   target_layer.set_weights(source_layer.get_weights())\n",
    "    else:\n",
    "        if function == 'localisation':\n",
    "            resnet_function = Resnet3DBuilder.build_resnet_34\n",
    "            \n",
    "        elif function == 'detection':\n",
    "            resnet_function = Resnet3DBuilder.build_resnet_18\n",
    "\n",
    "        detection_model = resnet_function(detection_input_shape, [3,3], mode=function, reg_factor=1e-8, use_shortcut=use_shortcut)\n",
    "        \n",
    "    detection_model.compile(optimizer='adam',\n",
    "                      loss=loss)\n",
    "\n",
    "    start_time = time.time()\n",
    "    get_best = utils.GetBest(monitor='val_loss', verbose=0, mode='min')\n",
    "\n",
    "    history = detection_model.fit_generator(train_generator, validation_data=validation_generator, verbose=1,\n",
    "                                  validation_steps=math.ceil(len(ct_data['test'][dataset])/batch_size),\n",
    "                                  steps_per_epoch=math.ceil(len(ct_data['train'][dataset])/batch_size), \n",
    "                                  epochs=total_epochs,\n",
    "                                callbacks=[get_best])\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    loss_hist = history.history['loss']\n",
    "    val_loss_hist = history.history['val_loss']\n",
    "    best_loss = np.amin(val_loss_hist)\n",
    "    best_mse = evaluate_detection_model(detection_model, dataset, function=function, use_radiomics=use_radiomics, modalities=modalities)\n",
    "    model_name = get_model_name()\n",
    "\n",
    "    with np.printoptions(precision=2, suppress=True):\n",
    "        print('loss:', np.array(loss_hist))\n",
    "        print('val_loss:', np.array(val_loss_hist))\n",
    "        print('best_loss:', best_loss)\n",
    "        print('best_mse:', best_mse)\n",
    "        \n",
    "    with open('Models/keras models/training_log.txt', 'a+') as log:\n",
    "        log.write('{} \\ntime {}\\nbest loss/mse: {} {}\\ntraining: {}\\nvalidation: {}\\n\\n'.format(\n",
    "            model_name, total_time, best_loss, best_mse, loss_hist, val_loss_hist))\n",
    "    \n",
    "    if save_model:\n",
    "        save_model_function('Models/keras models/{}'.format(model_name), detection_model)\n",
    "        \n",
    "    return detection_model\n",
    "    \n",
    "\n",
    "for i in range(4,7):\n",
    "    blyat_model = train_detection_model(dataset='BreastCancer', use_radiomics=i, total_epochs=200, save_model=False, function='localisation',\n",
    "                                       loss=losses.mean_squared_error)\n",
    "    reset_keras()\n",
    "    print('finished', i)\n",
    "\n",
    "    \n",
    "'''detection_model = train_detection_model(dataset='HeadNeckCancer', use_radiomics=None, total_epochs=100, save_model=True, function='detection',\n",
    "                                   loss=losses.mean_squared_error)\n",
    "\n",
    "transferring_model=detection_model\n",
    "detection_model = train_detection_model(dataset='BreastCancer', use_radiomics=None, total_epochs=100, save_model=True, function='detection',\n",
    "                                   loss=losses.mean_squared_error, transferring_model=transferring_model)\n",
    "reset_keras()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06149809  0.03975723  0.0166484  -0.15415365 -0.06149759]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for layer in detection_model.get_layer('lambda_5'):\\n    print(layer.name)\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(blyat_model.get_weights()[0][0,0,0,0,0:5])\n",
    "\n",
    "print(detection_model.get_weights()[0][0,0,0,0,0:5])\n",
    "'''for layer in detection_model.get_layer('lambda_5'):\n",
    "    print(layer.name)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values derived from NumpyAnalysis.ipynb\n",
    "#get_best_bounding_box()\n",
    "maximal_bounding_volume = np.array([32,32,32])\n",
    "\n",
    "#generator for (ct bounding box, pet bounding_box)-> mask bounding box prediction models\n",
    "def mask_bounding_box_predictor_generator(mode='train', dataset='HeadNeckCancer', shift_weight=0.7, shuffle=False, normalise=True, \n",
    "                                          use_radiomics=None, batch_size=4, region_radius=[16,16,16], pred_centres=None, modalities=['ct', 'pet']):\n",
    "\n",
    "            \n",
    "    def helper(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, mask_files, centres, sizes):\n",
    "        num_data = len(pet_files)\n",
    "        val_seeds = list(range(num_data))\n",
    "        volume_shape = np.load(ct_files[0]).shape\n",
    "        \n",
    "        nonlocal pred_centres\n",
    "        if pred_centres is None:\n",
    "            pred_centres = [None] * num_data\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            if shuffle:\n",
    "                z = list(zip(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, mask_files, pred_centres, val_seeds))\n",
    "                random.shuffle(z)\n",
    "                pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, mask_files, pred_centres, val_seeds = zip(*z)\n",
    "                                \n",
    "            ct_pet_batch = []\n",
    "            mask_batch = []\n",
    "            \n",
    "            for i in range(num_data):\n",
    "                \n",
    "                if mode == 'test': #make the validation generator generate predictable centres\n",
    "                    seed = val_seeds[i]\n",
    "                elif mode == 'train':\n",
    "                    seed = None\n",
    "                \n",
    "                #randomly sample centres/sizes around the true centre/sizes to generate bounding boxes\n",
    "                if pred_centres[i] is None:\n",
    "                    centre = shift_centre(centres[i], shift_weight, seed)\n",
    "                else:\n",
    "                    centre = pred_centres[i]\n",
    "                    \n",
    "                ct_pet = get_volume_input(modalities, ct_files[i], pet_files[i], ct_radiomics_files[i], pet_radiomics_files[i], \n",
    "                                          volume_shape, use_radiomics)\n",
    "                ct_pet = get_bounding_box(ct_pet, centre, np.array(region_radius))\n",
    "                mask = get_bounding_box(np.load(mask_files[i]), centre, np.array(region_radius))\n",
    "\n",
    "\n",
    "                if normalise:\n",
    "                    for axis in range(ct_pet.shape[-1]):\n",
    "                        ct_pet[:,:,:,axis] = normalise_volume(ct_pet[:,:,:,axis])\n",
    "                \n",
    "                ct_pet_batch.append(ct_pet)\n",
    "                mask_batch.append(mask)\n",
    "                \n",
    "                #print(centre - centres[i])\n",
    "\n",
    "                if len(ct_pet_batch) == batch_size:\n",
    "                    yield np.array(ct_pet_batch), np.array(mask_batch)\n",
    "                    ct_pet_batch.clear()\n",
    "                    mask_batch.clear()\n",
    "\n",
    "            if len(ct_pet_batch) > 0:\n",
    "                yield np.array(ct_pet_batch), np.array(mask_batch)\n",
    "                \n",
    "    return helper(pet_data[mode][dataset], ct_data[mode][dataset], pet_radiomics_data[mode][dataset], ct_radiomics_data[mode][dataset],\n",
    "                  mask_data[mode][dataset], centre_data[mode][dataset], size_data[mode][dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 32, 1) 2158.0\n",
      "(10, 32, 32, 32, 1) 3875.0\n",
      "(10, 32, 32, 32, 1) 5284.0\n",
      "(10, 32, 32, 32, 1) 6794.0\n",
      "(10, 32, 32, 32, 1) 7945.0\n",
      "(10, 32, 32, 32, 1) 8935.0\n",
      "(10, 32, 32, 32, 1) 9737.0\n",
      "(10, 32, 32, 32, 1) 11188.0\n",
      "(10, 32, 32, 32, 1) 13161.0\n",
      "(10, 32, 32, 32, 1) 14569.0\n",
      "(7, 32, 32, 32, 1) 15578.0\n",
      "mask proportion: 0.004443017121787383\n"
     ]
    }
   ],
   "source": [
    "#check to see if generator works\n",
    "batch_size = 10\n",
    "mode = 'train'\n",
    "dataset = 'BreastCancer'\n",
    "test_generator = mask_bounding_box_predictor_generator(mode, dataset, shuffle=True, normalise=False, shift_weight=0, \n",
    "                                                       batch_size=batch_size, use_radiomics=None, modalities=['pet'])\n",
    "num_bg = 0.0\n",
    "num_mask = 0.0\n",
    "\n",
    "for i in range(math.ceil(len(ct_data[mode][dataset]) / batch_size)):\n",
    "    ctpet, mask = next(test_generator)\n",
    "               \n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    counts = dict(zip(unique, counts))\n",
    "    \n",
    "    num_bg += counts[0]\n",
    "    num_mask += counts.get(1,0)\n",
    "    \n",
    "    print(ctpet.shape, num_mask)\n",
    "\n",
    "                      \n",
    "print('mask proportion:', num_mask / (num_bg + num_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 7.]\n",
      "[ 7.  7. 11.]\n",
      "[11. 11. 15.]\n",
      "[15. 15. 19.]\n",
      "[18. 19. 23.]\n"
     ]
    }
   ],
   "source": [
    "# calculates output shape given input parameters\n",
    "\n",
    "def deconv_calculator(input_size, kernels, stride):\n",
    "    input_size = np.array(input_size)\n",
    "    stride = np.array(stride)\n",
    "\n",
    "    for kernel in kernels:\n",
    "        \n",
    "        kernel = np.array(kernel)\n",
    "        input_size = np.ceil((input_size - 1) * stride + kernel)\n",
    "        print(input_size)\n",
    "\n",
    "deconv_calculator([3,3,3], [[3,3,5],\n",
    "                            [3,3,5],\n",
    "                            [5,5,5],\n",
    "                            [5,5,5],\n",
    "                           [4,5,5]], [1,1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.50200803 125.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#computes the weights where if they are applied to each item, taking the mean would yield 1\n",
    "#used for segmentation mask weighting\n",
    "def get_class_weight(mask_proportion=0.33, precision=1000):\n",
    "    \n",
    "    arr = np.empty(precision)\n",
    "    index = int(mask_proportion * precision)\n",
    "    arr[:index] = 1\n",
    "    arr[index:] = 0\n",
    "    weights = class_weight.compute_class_weight('balanced', [0,1], arr)\n",
    "    return weights\n",
    "\n",
    "class_weights = get_class_weight(0.004443017121787383) #proportion taken from above\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE                 |Almost_right |half right |all_wrong\n",
      "mult [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "jaccard_distance_loss 0.0\n",
      "binary_crossentropy [1.0174631e-07]\n",
      "binary_crossentropy_scaled [1.]\n",
      "weighted_binary_crossentropy 1.4002886e-06\n",
      "weighted_binary_crossentropy_scaled 1.0\n",
      "upsampled [[[[0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1.]\n",
      "   [2. 2. 3. 3.]\n",
      "   [2. 2. 3. 3.]]\n",
      "\n",
      "  [[0. 0. 1. 1.]\n",
      "   [0. 0. 1. 1.]\n",
      "   [2. 2. 3. 3.]\n",
      "   [2. 2. 3. 3.]]\n",
      "\n",
      "  [[4. 4. 5. 5.]\n",
      "   [4. 4. 5. 5.]\n",
      "   [6. 6. 7. 7.]\n",
      "   [6. 6. 7. 7.]]\n",
      "\n",
      "  [[4. 4. 5. 5.]\n",
      "   [4. 4. 5. 5.]\n",
      "   [6. 6. 7. 7.]\n",
      "   [6. 6. 7. 7.]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Accepts tensors; from https://github.com/keras-team/keras/issues/3611\n",
    "#Higher is better\n",
    "def dice_metric(y_true, y_pred, smooth=1):\n",
    "    y_pred = K.round(y_pred)\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "#From https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "def dice_coef(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    y_true = K.batch_flatten(y_true)\n",
    "    y_pred = K.batch_flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "#From https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    y_true = K.batch_flatten(y_true)\n",
    "    y_pred = K.batch_flatten(y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = K.mean((intersection + smooth) / (sum_ - intersection + smooth))\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "\n",
    "\n",
    "#from https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    zero_weight = class_weights[0]\n",
    "    one_weight = class_weights[1]\n",
    "    \n",
    "    # Calculate the binary crossentropy\n",
    "    b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Apply the weights\n",
    "    weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "    weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "    # Return the mean error\n",
    "    return K.mean(weighted_b_ce)\n",
    "\n",
    "def blyat():\n",
    "\n",
    "    # Test\n",
    "    # Test\n",
    "    print(\"TYPE                 |Almost_right |half right |all_wrong\")\n",
    "    y_true = np.array([[1,0,0,0,0,0,0,0,0,0,0]])\n",
    "    y_pred = np.array([[1,0,0,0,0,0,0,0,0,0,0]])\n",
    "    \n",
    "    r = (\n",
    "        K.variable(y_true) *\n",
    "        K.variable(y_pred)\n",
    "    ).eval(session=K.get_session())\n",
    "    print('mult',r)\n",
    "    \n",
    "    r = jaccard_distance_loss(\n",
    "        K.variable(y_true),\n",
    "        K.variable(y_pred),\n",
    "    ).eval(session=K.get_session())\n",
    "    print('jaccard_distance_loss',r)\n",
    "    #assert r[0]<r[1]\n",
    "    #assert r[1]<r[2]\n",
    "\n",
    "    r = keras.losses.binary_crossentropy(\n",
    "        K.variable(y_true),\n",
    "        K.variable(y_pred),\n",
    "    ).eval(session=K.get_session())\n",
    "    print('binary_crossentropy',r)\n",
    "    print('binary_crossentropy_scaled',r/r.max())\n",
    "    #assert r[0]<r[1]\n",
    "    #assert r[1]<r[2]\n",
    "    \n",
    "    r = weighted_binary_crossentropy(\n",
    "        K.variable(y_true),\n",
    "        K.variable(y_pred),\n",
    "    ).eval(session=K.get_session())\n",
    "    print('weighted_binary_crossentropy',r)\n",
    "    print('weighted_binary_crossentropy_scaled',r/r.max())\n",
    "    \n",
    "    ups = np.arange(8).reshape((1,2,2,2))\n",
    "    r = resnet3d4.upsample3d(2)(\n",
    "        K.variable(ups)\n",
    "    ).eval(session=K.get_session())\n",
    "    print('upsampled',r)\n",
    "    \n",
    "blyat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate segmentation model\n",
    "\n",
    "#Accepts numpy arrays\n",
    "#Higher is better\n",
    "def dice_eval(true_mask, pred_mask, pred_centres):\n",
    "    pred_mask = np.rint(pred_mask)\n",
    "    true_mask = np.rint(true_mask)\n",
    "    \n",
    "    true_values = np.unique(true_mask)\n",
    "    pred_values = np.unique(pred_mask)\n",
    "    \n",
    "    #when training, if both true and predicted masks are 0's this is the best prediction\n",
    "    #when evaluating, this indicates that the predicted centre is outside of the scope of the mask; worst prediction\n",
    "    if np.array_equal(true_values, [0]) and np.array_equal(pred_values, [0]):\n",
    "        if pred_centres is None:\n",
    "            return 1 \n",
    "        else:\n",
    "            return 0 \n",
    "    \n",
    "    pred_mask = pred_mask.flatten()\n",
    "    true_mask = true_mask.flatten()\n",
    "    ret = 1 - distance.dice(pred_mask, true_mask)\n",
    "    return ret\n",
    "\n",
    "#returns dice acc, dice std, and best image segmentation\n",
    "def evaluate_segmentation_model(dataset, model, mode='test', output_segmentation=False, fill_holes=False, use_radiomics=None,\n",
    "                                pred_centres=None, shift_weight=0.7, region_radius=[16,16,16], modalities=['ct', 'pet']):\n",
    "    \n",
    "    validation_generator = mask_bounding_box_predictor_generator(mode, dataset, shuffle=False, \n",
    "                                                                 use_radiomics=use_radiomics, normalise=True, batch_size=1,\n",
    "                                                                 pred_centres=pred_centres, shift_weight=shift_weight, modalities=modalities)\n",
    "    dices = []\n",
    "    max_acc = 0\n",
    "    max_patient = ''\n",
    "        \n",
    "    for i in range(len(ct_data[mode][dataset])):\n",
    "        ctpet, true_mask = next(validation_generator)\n",
    "        pred_mask = model.predict(ctpet)[0]\n",
    "        \n",
    "        if pred_centres is not None: #shift the true bounding box so that it is centres with pred_centres\n",
    "            true_mask = get_bounding_box(np.load(mask_data[mode][dataset][i]), pred_centres[i], np.array(region_radius))\n",
    "        else:\n",
    "            true_mask = true_mask[0]\n",
    "        \n",
    "        if fill_holes:\n",
    "            pred_mask = binary_fill_holes(pred_mask)\n",
    "        \n",
    "        dice_index = dice_eval(true_mask, pred_mask, pred_centres)\n",
    "        dices.append(dice_index)\n",
    "                \n",
    "        if dice_index > max_acc:\n",
    "            max_acc = dice_index\n",
    "            max_patient = ct_data[mode][dataset][i]\n",
    "        \n",
    "        if output_segmentation:\n",
    "            patient_name = os.path.splitext(os.path.basename(pet_data[mode][dataset][i]))[0]\n",
    "            mask_radius = 0.5 * np.array((32, 32, 32))\n",
    "            utils.output_numpy_mask_to_nrrd(patient_name, np.rint(pred_mask), centre_data[mode][dataset][i], mask_radius, dataset)\n",
    "            utils.output_numpy_mask_to_nrrd(patient_name, true_mask, centre_data[mode][dataset][i], mask_radius, dataset, filename_tag='-true')\n",
    "            \n",
    "            \n",
    "    return np.average(dices), np.std(dices), os.path.basename(max_patient), max_acc\n",
    "\n",
    "#print('dice accuracy, dice slice-by-slice accuracy, most accurate:')\n",
    "#print(evaluate_segmentation_model('BreastCancer', segmentation_model, 'test', output_segmentation=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for reloading models\n",
    "custom_objects={'jaccard_distance_loss' : jaccard_distance_loss,\n",
    "                                    'dice_coef_loss': dice_coef_loss,\n",
    "                                          'dice_metric': dice_metric,\n",
    "                'weighted_binary_crossentropy' : weighted_binary_crossentropy,\n",
    "                                          'dice_coef': dice_coef,\n",
    "                                       'conv4d' : conv4d.conv4d,\n",
    "                                    'l2': l2,\n",
    "                                    'Resnet3DBuilder': Resnet3DBuilder,\n",
    "                                    'tf' : tf,\n",
    "                                    'upsample3d_helper' : resnet3d4.upsample3d_helper}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "dataset = 'BreastCancer'\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.42391753  6.94835554 20.97476426]\n"
     ]
    }
   ],
   "source": [
    "#CANT LOAD THIS SHIT CONV4D DOES NOT LOAD WEIGHTS PROPERLY BLYAAAAAT\n",
    "#detection_model = load_model_function('/home/jzhe0882/Models/keras models/detection BreastCancer batch=4 cycles=1 shuffle=True normalise=True use_radiomics=None transferred=False mean_squared_error')\n",
    "\n",
    "#detection model test\n",
    "test_generator = detection_generator(mode=mode, dataset=dataset, shuffle=False, normalise=True, batch_size=batch_size, use_radiomics=None)    \n",
    "detected_centres = detection_model.predict_generator(test_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size))   \n",
    "centre_mse =  mean_squared_error(detected_centres, centre_data[mode][dataset], multioutput='raw_values')\n",
    "\n",
    "print(centre_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save detected centres\n",
    "\n",
    "test_generator = detection_generator(mode=mode, dataset=dataset, shuffle=False, normalise=True, batch_size=batch_size, use_radiomics=None)    \n",
    "detected_centres = detection_model.predict_generator(test_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size))   \n",
    "centre_mse =  mean_squared_error(detected_centres, centre_data[mode][dataset], multioutput='raw_values')\n",
    "\n",
    "\n",
    "print(centre_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.42391753  6.94835554 20.97476426]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#np.save('Models/keras models/detected_centres.npy', detected_centres)\n",
    "\n",
    "detected_centres = np.load('Models/keras models/detected_centres.npy')\n",
    "\n",
    "centre_mse =  mean_squared_error(blyat_centres, centre_data[mode][dataset], multioutput='raw_values')\n",
    "print(centre_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(unsuccessfully) successively refine predicted centres\n",
    "\n",
    "loops=100\n",
    "\n",
    "def successive_refinement(detected_centres, loops=100):\n",
    "    successive_refined_centres = np.empty((loops, len(centre_data[mode][dataset]), 3))\n",
    "    current_prediction = detected_centres\n",
    "    \n",
    "    for i in range(loops):\n",
    "        test_generator = localisation_generator(mode=mode, dataset=dataset, batch_size=batch_size, \n",
    "                                                shuffle=False, normalise=True, use_radiomics=None, pred_centres=current_prediction)\n",
    "        current_prediction = detection_model.predict_generator(test_generator, steps=math.ceil(len(ct_data[mode][dataset])/batch_size))\n",
    "        successive_refined_centres[i] = current_prediction\n",
    "        \n",
    "        print(mean_squared_error(current_prediction, centre_data[mode][dataset], multioutput='raw_values'))\n",
    "    return successive_refined_centres\n",
    "\n",
    "refined_centres = successive_refinement(detected_centres)\n",
    "np.save('Models/keras models/refined_centres.npy', refined_centres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save refinement losses\n",
    "\n",
    "refined_centres = np.load('Models/keras models/refined_centres.npy')\n",
    "refined_centres_mses = np.empty((loops, 3))\n",
    "refined_centres_diffs = np.empty((loops, len(centre_data[mode][dataset]), 3))\n",
    "\n",
    "for i in range(len(refined_centres)):\n",
    "    centre_mse =  mean_squared_error(refined_centres[i], centre_data[mode][dataset], multioutput='raw_values')\n",
    "    refined_centres_mses[i] = centre_mse\n",
    "    refined_centres_diffs[i] = centre_data[mode][dataset] - refined_centres[i]\n",
    "    \n",
    "np.save('Models/keras models/refined_centres_mses.npy', refined_centres_mses)\n",
    "np.save('Models/keras models/refined_centres_diffs.npy', refined_centres_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_segmentation_model(dataset, segmentation_model, mode, output_segmentation=False, fill_holes=False, \n",
    "                                                  use_radiomics=None, pred_centres=refined_centres[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how shift_weight varies each prediction\n",
    "\n",
    "dice_indices = np.empty((100))\n",
    "for i in range(100):\n",
    "    \n",
    "    blyat_centres = []\n",
    "    for j in range(len(centre_data[mode][dataset])):\n",
    "        centre = shift_centre(centre_data[mode][dataset][j], shift_weight=0.01*i, seed=j)\n",
    "        blyat_centres.append(centre)\n",
    "        \n",
    "    dice_indices[i],_,_,_ = evaluate_segmentation_model(dataset, segmentation_model, mode=mode, output_segmentation=False, fill_holes=False, \n",
    "                                                  use_radiomics=None, pred_centres=blyat_centres)\n",
    "    print(dice_indices[i])\n",
    "    \n",
    "np.save('Models/keras models/dice_indices.npy', dice_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detection_model.get_layer(index=-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old validation generator for resizing stuff\n",
    "maximal_bounding_volume = np.array([32,32,32])\n",
    "\n",
    "#generator for (ct bounding box, pet bounding_box)-> mask bounding box prediction models\n",
    "def mask_bounding_box_predictor_generator(mode='train', dataset='HeadNeckCancer', shift_centres=True, shift_sizes=True, shift_weight=0.7, \n",
    "                                          shuffle=False, normalise=True, use_radiomics=True, batch_size=4, region_radius=[16,16,16]):\n",
    "\n",
    "    def shift_centre(centre):\n",
    "        centre = np.random.randn(3) * shift_weight * np.array([20.4,4.6,21.2]) + centre #std taken from NumpyAnalysis\n",
    "        centre = np.maximum(centre, [0,0,0]) #keep centre within image bounds\n",
    "        centre = np.minimum(centre, volume_size)\n",
    "        return centre.astype(int)\n",
    "    \n",
    "    def shift_size(size):\n",
    "        size = np.random.randn(3) * shift_weight *  np.array([3.6,3,5.7]) + size #std taken from NumpyAnalysis\n",
    "        size = np.maximum(size, [1,1,1]) #keep size to > 1\n",
    "        size = np.minimum(size, maximal_bounding_volume)\n",
    "        return size\n",
    "        \n",
    "    def get_resized_bounding_box(source_volume, centre, size, target_size, mode):\n",
    "        bounding_box = get_bounding_box(source_volume, centre, 0.5 * size)\n",
    "        return resize_bounding_box(bounding_box, target_size, mode)\n",
    "            \n",
    "    def helper(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, mask_files, centres, sizes, \n",
    "               shift_centres, shift_sizes, shuffle, normalise, use_radiomics, batch_size):\n",
    "        while True:\n",
    "\n",
    "            if shuffle:\n",
    "                z = list(zip(pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, mask_files))\n",
    "                random.shuffle(z)\n",
    "                pet_files, ct_files, pet_radiomics_files, ct_radiomics_files, centres, sizes, mask_files = zip(*z)\n",
    "                                \n",
    "            ct_pet_batch = []\n",
    "            mask_batch = []\n",
    "            \n",
    "            for i in range(len(pet_files)):\n",
    "                \n",
    "                #randomly sample centres/sizes around the true centre/sizes to generate bounding boxes\n",
    "                if shift_centres:\n",
    "                    centre = shift_centre(centres[i], shift_weight,)\n",
    "                else:\n",
    "                    centre = centres[i]\n",
    "\n",
    "                if shift_sizes:\n",
    "                    size = shift_size(sizes[i])\n",
    "                else:\n",
    "                    size = sizes[i]\n",
    "  \n",
    "\n",
    "                ct = get_resized_bounding_box(np.load(ct_files[i]), centre, size, maximal_bounding_volume, 'ct')\n",
    "                pet = get_resized_bounding_box(np.load(pet_files[i]), centre, size, maximal_bounding_volume, 'pet')\n",
    "                mask = get_resized_bounding_box(np.load(mask_files[i]), centre, size, maximal_bounding_volume, 'mask')\n",
    "                \n",
    "                ct_pet = zip_np_volumes(ct, pet)\n",
    "                \n",
    "                if use_radiomics:\n",
    "                    ct_radiomics = np.load(ct_radiomics_files[i])\n",
    "                    pet_radiomics = np.load(pet_radiomics_files[i])\n",
    "                    \n",
    "                    for axis in range(ct_radiomics.shape[-1]):\n",
    "                        ct_radiomic_feature = get_resized_bounding_box(ct_radiomics[:,:,:,axis], centre, size, maximal_bounding_volume, 'ct')\n",
    "                        pet_radiomic_feature = get_resized_bounding_box(pet_radiomics[:,:,:,axis], centre, size, maximal_bounding_volume, 'pet')\n",
    "                        ct_radiomic_feature = np.expand_dims(ct_radiomic_feature, axis=-1)\n",
    "                        pet_radiomic_feature = np.expand_dims(pet_radiomic_feature, axis=-1)\n",
    "                        \n",
    "                        ct_pet = np.concatenate((ct_pet, ct_radiomic_feature, pet_radiomic_feature), axis=-1)\n",
    "                \n",
    "                if normalise:\n",
    "                    for axis in range(ct_pet.shape[-1]):\n",
    "                        ct_pet[:,:,:,axis] = normalise_volume(ct_pet[:,:,:,axis])\n",
    "                \n",
    "                ct_pet_batch.append(ct_pet)\n",
    "                mask_batch.append(mask)\n",
    "\n",
    "                if len(ct_pet_batch) == batch_size:\n",
    "                    yield np.array(ct_pet_batch), np.array(mask_batch)\n",
    "                    ct_pet_batch.clear()\n",
    "                    mask_batch.clear()\n",
    "\n",
    "            if len(ct_pet_batch) > 0:\n",
    "                yield np.array(ct_pet_batch), np.array(mask_batch)\n",
    "                \n",
    "    return helper(pet_data[mode][dataset], ct_data[mode][dataset], pet_radiomics_data[mode][dataset], ct_radiomics_data[mode][dataset],\n",
    "                  mask_data[mode][dataset], centre_data[mode][dataset], size_data[mode][dataset],\n",
    "                 shift_centres, shift_sizes, shuffle, normalise, use_radiomics, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 2.]]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blyat = np.zeros((1,1,1))\n",
    "print(blyat)\n",
    "np.append(blyat, [[[2]]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
